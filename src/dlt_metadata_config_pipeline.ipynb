{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip config set global.proxy http://usaeast-proxy.us.experian.eeca:9595\n",
    "%pip install pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9198e987-5606-403d-9f6d-8f14e6a4017f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import yaml\n",
    "import logging\n",
    "from logging import Formatter\n",
    "from typing import Any, Callable, Dict, List, Union, Optional, Dict, Literal\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "from dataclasses import asdict, dataclass, field\n",
    "from pyspark.sql import DataFrame, Row, SparkSession\n",
    "from datetime import datetime\n",
    "import dlt\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"config_path\", f\"/Workspace/Users/theodore.kop@databricks.com/.bundle/experian_dlt/dev/files/resources/pipeline_config/pipeline_tbls_config.yml\")\n",
    "config_path = dbutils.widgets.get(\"config_path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Logger = logging.Logger\n",
    "\n",
    "def create_logger(\n",
    "    name: str = __name__, level: Union[int, str] = logging.DEBUG\n",
    ") -> Logger:\n",
    "    \"\"\"\n",
    "    Create a logger with the specified name and log level.\n",
    "\n",
    "    Args:\n",
    "        name (str): The name of the logger. Defaults to the current module name.\n",
    "        level (Union[int, str]): The log level. Can be an integer or a string.\n",
    "            Defaults to logging.DEBUG.\n",
    "\n",
    "    Returns:\n",
    "        Logger: The created logger instance.\n",
    "    \"\"\"\n",
    "    logger: Logger = logging.getLogger(name)\n",
    "    level = level if isinstance(level, int) else logging.getLevelName(level)\n",
    "    logger.setLevel(level)\n",
    "\n",
    "    formatter = Formatter(\n",
    "        \"%(asctime)s - %(name)s - %(pathname)s:%(lineno)d - %(levelname)s - %(message)s\"\n",
    "    )\n",
    "    handler: logging.StreamHandler = logging.StreamHandler()\n",
    "    handler.setLevel(level)\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DestinationType = Literal[\n",
    "    \"view\",\n",
    "    \"table\",\n",
    "]\n",
    "\n",
    "SourceFormat = Literal[\n",
    "    \"cloudFiles\", \n",
    "    \"kafka\", \n",
    "    \"csv\", \n",
    "    \"json\", \n",
    "    \"parquet\", \n",
    "    \"avro\", \n",
    "    \"orc\", \n",
    "    \"delta\",\n",
    "    \"dlt\"\n",
    "]\n",
    "\n",
    "ReadOptions = Dict[str, str]\n",
    "\n",
    "TableProperties = Dict[str, str]\n",
    "\n",
    "Tags = Dict[str, str]\n",
    "\n",
    "SparkConf = Dict[str, str]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ApplyChanges:\n",
    "\n",
    "    sequence_by: str\n",
    "    where: Optional[str] = None\n",
    "    ignore_null_updates: Optional[bool] = None\n",
    "    apply_as_deletes: Optional[str] = None\n",
    "    apply_as_truncates: Optional[str] = None\n",
    "    column_list: Optional[List[str]] = None\n",
    "    except_column_list: Optional[List[str]] = None\n",
    "    stored_as_scd_type: int = 1\n",
    "    track_history_column_list: Optional[List[str]] = None\n",
    "    track_history_except_column_list: Optional[List[str]] = None\n",
    "    flow_name: Optional[str] = None\n",
    "    ignore_null_updates_column_list: Optional[List[str]] = None\n",
    "    ignore_null_updates_except_column_list: Optional[List[str]] = None\n",
    "\n",
    "    @classmethod\n",
    "    def spark_schema(cls) -> T.StructType:\n",
    "        \"\"\"\n",
    "        Returns the Spark schema for the Delta Live entity expectations.\n",
    "\n",
    "        Returns:\n",
    "            T.StructType: The Spark schema for the Delta Live entity expectations.\n",
    "        \"\"\"\n",
    "        schema: T.StructType = T.StructType(\n",
    "            [\n",
    "                T.StructField(\"sequence_by\", T.StringType(), True),\n",
    "                T.StructField(\"where\", T.StringType(), True),\n",
    "                T.StructField(\"ignore_null_updates\", T.BooleanType(), True),\n",
    "                T.StructField(\"apply_as_deletes\", T.StringType(), True),\n",
    "                T.StructField(\"apply_as_truncates\", T.StringType(), True),\n",
    "                T.StructField(\"column_list\", T.ArrayType(T.StringType()), True),\n",
    "                T.StructField(\"except_column_list\", T.ArrayType(T.StringType()), True),\n",
    "                T.StructField(\"stored_as_scd_type\", T.IntegerType(), True),\n",
    "                T.StructField(\n",
    "                    \"track_history_column_list\", T.ArrayType(T.StringType()), True\n",
    "                ),\n",
    "                T.StructField(\n",
    "                    \"track_history_except_column_list\",\n",
    "                    T.ArrayType(T.StringType()),\n",
    "                    True,\n",
    "                ),\n",
    "                T.StructField(\"flow_name\", T.StringType(), True),\n",
    "                T.StructField(\n",
    "                    \"ignore_null_updates_column_list\", T.ArrayType(T.StringType()), True\n",
    "                ),\n",
    "                T.StructField(\n",
    "                    \"ignore_null_updates_except_column_list\",\n",
    "                    T.ArrayType(T.StringType()),\n",
    "                    True,\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        return schema\n",
    "\n",
    "#    def copy(self) -> ApplyChanges:\n",
    "#        \"\"\"\n",
    "#        Creates a copy of the Delta Live entity apply changes.\n",
    "#\n",
    "#        Returns:\n",
    "#            ApplyChanges: The copy of the Delta Live entity apply changes.\n",
    "#        \"\"\"\n",
    "#        return ApplyChanges(**self.to_dict())\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Converts the Delta Live entity apply changes to a dictionary.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, any]: The dictionary representation of the Delta Live entity apply changes.\n",
    "        \"\"\"\n",
    "        return asdict(self)\n",
    "\n",
    "@dataclass\n",
    "class Expectations:\n",
    "    \"\"\"\n",
    "    Represents the expectations for a Delta Live entity.\n",
    "\n",
    "    Attributes:\n",
    "        expect_all (Dict[str, str], optional): The expect all expectations. Defaults to an empty dictionary.\n",
    "        expect_all_or_drop (Dict[str, str], optional): The expect all or drop expectations. Defaults to an empty dictionary.\n",
    "        expect_all_or_fail (Dict[str, str], optional): The expect all or fail expectations. Defaults to an empty dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    expect_all: Dict[str, str] = field(default_factory=dict)\n",
    "    expect_all_or_drop: Dict[str, str] = field(default_factory=dict)\n",
    "    expect_all_or_fail: Dict[str, str] = field(default_factory=dict)\n",
    "\n",
    "    @classmethod\n",
    "    def spark_schema(cls) -> T.StructType:\n",
    "        \"\"\"\n",
    "        Returns the Spark schema for the Delta Live entity expectations.\n",
    "\n",
    "        Returns:\n",
    "            T.StructType: The Spark schema for the Delta Live entity expectations.\n",
    "        \"\"\"\n",
    "        schema: T.StructType = T.StructType(\n",
    "            [\n",
    "                T.StructField(\n",
    "                    \"expect_all\", T.MapType(T.StringType(), T.StringType()), True\n",
    "                ),\n",
    "                T.StructField(\n",
    "                    \"expect_all_or_drop\",\n",
    "                    T.MapType(T.StringType(), T.StringType()),\n",
    "                    True,\n",
    "                ),\n",
    "                T.StructField(\n",
    "                    \"expect_all_or_fail\",\n",
    "                    T.MapType(T.StringType(), T.StringType()),\n",
    "                    True,\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        return schema\n",
    "\n",
    "#    def copy(self) -> Expectations:\n",
    "#        \"\"\"\n",
    "#        Creates a copy of the Delta Live entity expectations.\n",
    "#\n",
    "#        Returns:\n",
    "#            DeltaLiveEntityExpectations: The copy of the Delta Live entity expectations.\n",
    "#        \"\"\"\n",
    "#        return Expectations(**self.to_dict())\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Converts the Delta Live entity expectations to a dictionary.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, any]: The dictionary representation of the Delta Live entity expectations.\n",
    "        \"\"\"\n",
    "        return asdict(self)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DeltaLiveEntity:\n",
    "    \"\"\"\n",
    "    Represents a Delta Live entity.\n",
    "\n",
    "    Attributes:\n",
    "        entity_id (str): The ID of the entity.\n",
    "        source (str): The source of the entity.\n",
    "        destination (str): The destination of the entity.\n",
    "        destination_type (DestinationType, optional): The type of the destination. Defaults to \"table\".\n",
    "        source_format (SourceFormat, optional): The format of the source. Defaults to \"cloudFiles\".\n",
    "        is_streaming (bool, optional): Indicates if the entity is streaming. Defaults to True.\n",
    "        primary_keys (List[str], optional): The primary keys. Defaults to an empty list.\n",
    "        source_schema (str, optional): The schema of the source. Defaults to None.\n",
    "        select_expr (List[str], optional): The list of select expressions. Defaults to an empty list.\n",
    "        read_options (ReadOptions, optional): The read options for the entity. Defaults to an empty dictionary.\n",
    "        table_properties (TableProperties, optional): The properties of the table. Defaults to an empty dictionary.\n",
    "        tags (Tags, optional): The tags associated with the entity. Defaults to an empty dictionary.\n",
    "        spark_conf (SparkConf, optional): The Spark configuration for the entity. Defaults to an empty dictionary.\n",
    "        partition_cols (List[str], optional): The partition columns of the entity. Defaults to an empty list.\n",
    "        group (str, optional): The group of the entity. Defaults to None.\n",
    "        comment (str, optional): The comment for the entity. Defaults to None.\n",
    "        id (str, optional): The ID of the entity. Defaults to None.\n",
    "        created_ts (datetime, optional): The timestamp when the entity was created. Defaults to None.\n",
    "        expired_ts (datetime, optional): The timestamp when the entity expired. Defaults to None.\n",
    "        created_by (str, optional): The user who created the entity. Defaults to None.\n",
    "        is_enabled (bool, optional): Indicates if the entity is enabled. Defaults to True.\n",
    "        is_latest (bool, optional): Indicates if the entity is the latest version. Defaults to None.\n",
    "        hash (bool, optional): The hash value of the entity. Defaults to None.\n",
    "        expectations (DeltaLiveEntityExpectations, optional): The expectations for the entity. Valid keys are: expect_all, expect_all_or_drop, expect_all_or_fail. Defaults to an empty dictionary.\n",
    "        is_quarantined (bool, optional): Indicates if the entity is will quarantine invalid records. Defaults to False.\n",
    "        apply_changes (ApplyChanges, optional): The apply CDC changes for the entity. Defaults to an empty dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    entity_id: str\n",
    "    source: str\n",
    "    destination: str\n",
    "    destination_type: DestinationType = field(default=\"table\")\n",
    "    source_format: SourceFormat = field(default=\"cloudFiles\")\n",
    "    is_streaming: bool = True\n",
    "    primary_keys: List[str] = field(default_factory=list)\n",
    "    source_schema: Optional[str] = None\n",
    "    select_expr: List[str] = field(default_factory=list)\n",
    "    read_options: ReadOptions = field(default_factory=dict)\n",
    "    table_properties: TableProperties = field(default_factory=dict)\n",
    "    tags: Tags = field(default_factory=dict)\n",
    "    spark_conf: SparkConf = field(default_factory=dict)\n",
    "    partition_cols: List[str] = field(default_factory=list)\n",
    "    group: Optional[str] = None\n",
    "    comment: Optional[str] = None\n",
    "    id: Optional[str] = None\n",
    "    created_ts: Optional[datetime] = None\n",
    "    expired_ts: Optional[datetime] = None\n",
    "    created_by: Optional[str] = None\n",
    "    modified_by: Optional[str] = None\n",
    "    is_enabled: bool = True\n",
    "    is_latest: Optional[bool] = None\n",
    "    hash: Optional[bool] = None\n",
    "    expectations: Union[Expectations, Row, Dict[str, Dict[str, Any]], None] = None\n",
    "    apply_changes: Union[ApplyChanges, Row, Dict[str, Any], None] = None\n",
    "    is_quarantined: bool = False\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"\n",
    "        Post-initialization method.\n",
    "        Performs additional initialization logic after the object is created.\n",
    "        \"\"\"\n",
    "        if self.source_format in [\"cloudFiles\", \"kafka\"]:\n",
    "            self.is_streaming = True\n",
    "        if self.source_format in [\"parquet\", \"csv\", \"json\", \"avro\", \"orc\"]:\n",
    "            self.is_streaming = False\n",
    "\n",
    "        self.group = None if self.group == \"\" else self.group\n",
    "        self.tags = {} if self.tags is None else self.tags\n",
    "        self.spark_conf = {} if self.spark_conf is None else self.spark_conf\n",
    "        self.partition_cols = [] if self.partition_cols is None else self.partition_cols\n",
    "        self.table_properties = (\n",
    "            {} if self.table_properties is None else self.table_properties\n",
    "        )\n",
    "        self.read_options = {} if self.read_options is None else self.read_options\n",
    "\n",
    "        self.expectations = (\n",
    "            Expectations() if self.expectations is None else self.expectations\n",
    "        )\n",
    "\n",
    "        if self.expectations is not None and not isinstance(\n",
    "            self.expectations, Expectations\n",
    "        ):\n",
    "            if isinstance(self.expectations, Row):\n",
    "                self.expectations = Expectations(**self.expectations.asDict())\n",
    "            elif isinstance(self.expectations, dict):\n",
    "                self.expectations = Expectations(**self.expectations)\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Invalid type for expectations. Must be a Row or a dictionary. Found: {type(self.expectations)}\"\n",
    "                )\n",
    "\n",
    "        if self.apply_changes is not None and not isinstance(\n",
    "            self.apply_changes, ApplyChanges\n",
    "        ):\n",
    "            if isinstance(self.apply_changes, Row):\n",
    "                self.apply_changes = ApplyChanges(**self.apply_changes.asDict())\n",
    "            elif isinstance(self.apply_changes, dict):\n",
    "                self.apply_changes = ApplyChanges(**self.apply_changes)\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Invalid type for apply_changes. Must be a Row or a dictionary. Found: {type(self.apply_changes)}\"\n",
    "                )\n",
    "\n",
    "    @classmethod\n",
    "    def spark_schema(cls) -> T.StructType:\n",
    "        \"\"\"\n",
    "        Returns the Spark schema for the Delta Live entity.\n",
    "\n",
    "        Returns:\n",
    "            T.StructType: The Spark schema for the Delta Live entity.\n",
    "        \"\"\"\n",
    "        schema: T.StructType = T.StructType(\n",
    "            [\n",
    "                T.StructField(\"entity_id\", T.StringType(), True),\n",
    "                T.StructField(\"source\", T.StringType(), True),\n",
    "                T.StructField(\"destination\", T.StringType(), True),\n",
    "                T.StructField(\"destination_type\", T.StringType(), True),\n",
    "                T.StructField(\"source_format\", T.StringType(), True),\n",
    "                T.StructField(\"is_streaming\", T.BooleanType(), True),\n",
    "                T.StructField(\"primary_keys\", T.ArrayType(T.StringType()), True),\n",
    "                T.StructField(\"source_schema\", T.StringType(), True),\n",
    "                T.StructField(\"select_expr\", T.ArrayType(T.StringType()), True),\n",
    "                T.StructField(\n",
    "                    \"read_options\", T.MapType(T.StringType(), T.StringType()), True\n",
    "                ),\n",
    "                T.StructField(\n",
    "                    \"table_properties\", T.MapType(T.StringType(), T.StringType()), True\n",
    "                ),\n",
    "                T.StructField(\"tags\", T.MapType(T.StringType(), T.StringType()), True),\n",
    "                T.StructField(\n",
    "                    \"spark_conf\", T.MapType(T.StringType(), T.StringType()), True\n",
    "                ),\n",
    "                T.StructField(\"partition_cols\", T.ArrayType(T.StringType()), True),\n",
    "                T.StructField(\"group\", T.StringType(), True),\n",
    "                T.StructField(\"comment\", T.StringType(), True),\n",
    "                T.StructField(\"id\", T.StringType(), True),\n",
    "                T.StructField(\"created_ts\", T.TimestampType(), True),\n",
    "                T.StructField(\"expired_ts\", T.TimestampType(), True),\n",
    "                T.StructField(\"created_by\", T.StringType(), True),\n",
    "                T.StructField(\"modified_by\", T.StringType(), True),\n",
    "                T.StructField(\"is_enabled\", T.BooleanType(), True),\n",
    "                T.StructField(\"is_latest\", T.BooleanType(), True),\n",
    "                T.StructField(\"hash\", T.IntegerType(), True),\n",
    "                T.StructField(\n",
    "                    \"expectations\",\n",
    "                    Expectations.spark_schema(),\n",
    "                    True,\n",
    "                ),\n",
    "                T.StructField(\n",
    "                    \"apply_changes\",\n",
    "                    ApplyChanges.spark_schema(),\n",
    "                    True,\n",
    "                ),\n",
    "                T.StructField(\"is_quarantined\", T.BooleanType(), True),\n",
    "            ]\n",
    "        )\n",
    "        return schema\n",
    "\n",
    "#    def copy(self) -> DeltaLiveEntity:\n",
    "#        \"\"\"\n",
    "#        Creates a copy of the Delta Live entity.\n",
    "#\n",
    "#        Returns:\n",
    "#            DeltaLiveEntity: The copy of the Delta Live entity.\n",
    "#        \"\"\"\n",
    "#        return DeltaLiveEntity(**self.to_dict())\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Converts the Delta Live entity to a dictionary.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, any]: The dictionary representation of the Delta Live entity.\n",
    "        \"\"\"\n",
    "        return asdict(self)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUARANTINE_COL: str = \"is_quarantined\"\n",
    "\n",
    "def can_quarantine(entity: DeltaLiveEntity) -> bool:\n",
    "    expect_all: Dict[str, str] = entity.expectations.expect_all\n",
    "    quarantine: bool = entity.is_quarantined and bool(expect_all) and not has_scd(entity)\n",
    "    logger.debug(f\"Can quarantine: {quarantine}\")\n",
    "    return quarantine\n",
    "\n",
    "\n",
    "def quarantine_rules(entity: DeltaLiveEntity) -> str:\n",
    "    expect_all: Dict[str, str] = entity.expectations.expect_all\n",
    "    rules: str = (\n",
    "        \"NOT({0})\".format(\" AND \".join(expect_all.values()))\n",
    "        if can_quarantine(entity)\n",
    "        else \"1=0\"\n",
    "    )\n",
    "    logger.debug(f\"Quarantine rules: {rules}\")\n",
    "    return rules\n",
    "\n",
    "def has_scd(entity: DeltaLiveEntity) -> bool:\n",
    "    scd: bool = bool(entity.primary_keys) and bool(entity.apply_changes)\n",
    "    return scd\n",
    "\n",
    "def load_entities_from_yaml(yaml_file: str) -> List[DeltaLiveEntity]:\n",
    "    \"\"\"\n",
    "    Reads a YAML file and creates a list of DeltaLiveEntity objects.\n",
    "    \n",
    "    Args:\n",
    "        yaml_file (str): Path to the YAML file containing the entities.\n",
    "    \n",
    "    Returns:\n",
    "        List[DeltaLiveEntity]: A list of DeltaLiveEntity objects.\n",
    "    \"\"\"\n",
    "    with open(yaml_file, 'r') as file:\n",
    "        data = yaml.safe_load(file)\n",
    "    \n",
    "    entities = []\n",
    "    for entity_data in data['delta_live_store']:\n",
    "        entity = DeltaLiveEntity(**entity_data)\n",
    "        entities.append(entity)\n",
    "    \n",
    "    return entities\n",
    "\n",
    "def run() -> None:\n",
    "  \"\"\"\n",
    "  Runs the pipeline, generating tables/views in the yml file.\n",
    "  \"\"\"\n",
    "  logger.info(f\"Running pipeline\")\n",
    "  entities = load_entities_from_yaml(config_path)\n",
    "  for entity in entities:\n",
    "    generate(entity)\n",
    "\n",
    "def generate(entity: DeltaLiveEntity) -> None:\n",
    "  \"\"\"\n",
    "  Generates a table or view for the specified entity.\n",
    "\n",
    "  Args:\n",
    "      entity: The DeltaLiveEntity instance representing the entity to generate for.\n",
    "  \"\"\"\n",
    "  match entity.destination_type:\n",
    "    case \"table\":\n",
    "      generate_table(entity)\n",
    "    case \"view\":\n",
    "      generate_view(entity)\n",
    "    case _:\n",
    "      raise ValueError(\n",
    "        f\"Unsupported destination type: {entity.destination_type}\"\n",
    "      )\n",
    "\n",
    "def generate_table(entity: DeltaLiveEntity) -> None:\n",
    "  \"\"\"\n",
    "  Generates a table for the specified entity.\n",
    "  Args:\n",
    "      entity: The DeltaLiveEntity instance representing the entity to generate a table for.\n",
    "  \"\"\"\n",
    "  logger.info(f\"Generating table for entity: {entity.entity_id}\")\n",
    "\n",
    "  partition_cols: List[str] = entity.partition_cols\n",
    "  name: str = entity.destination\n",
    "  quarantine_name: str = f\"{name}_quarantine\"\n",
    "  invalid_name: str = f\"{name}_invalid\"\n",
    "\n",
    "  if can_quarantine(entity):\n",
    "    _create_quarantine_tables(\n",
    "      valid_name=name,\n",
    "      invalid_name=invalid_name,\n",
    "      quarantine_name=quarantine_name,\n",
    "      entity=entity\n",
    "    )\n",
    "\n",
    "    name = quarantine_name\n",
    "    partition_cols = [QUARANTINE_COL] + partition_cols\n",
    "\n",
    "  if has_scd(entity):\n",
    "    _create_scd_table(name, partition_cols, entity)\n",
    "  else:\n",
    "    _create_table(name, partition_cols, entity)\n",
    "\n",
    "\n",
    "def generate_view(entity: DeltaLiveEntity) -> None:\n",
    "  \"\"\"\n",
    "  Generates a view for the specified entity.\n",
    "\n",
    "  Args:\n",
    "      entity: The DeltaLiveEntity instance representing the entity to generate a view for.\n",
    "  \"\"\"\n",
    "  logger.info(f\"Generating view for entity: {entity.entity_id}\")\n",
    "\n",
    "  entity_expectations: Expectations = entity.expectations\n",
    "  has_pipeline_dependency: bool = entity.source_format == \"dlt\"\n",
    "  @dlt.view(\n",
    "      name=entity.destination,\n",
    "      comment=entity.comment,\n",
    "      spark_conf=entity.spark_conf,\n",
    "  )\n",
    "  @dlt.expect_all(expectations=entity_expectations.expect_all)\n",
    "  @dlt.expect_all_or_drop(expectations=entity_expectations.expect_all_or_drop)\n",
    "  @dlt.expect_all_or_fail(expectations=entity_expectations.expect_all_or_fail)\n",
    "  def _():\n",
    "    df: DataFrame = None\n",
    "    if entity.is_streaming:\n",
    "      df = create_streaming(entity, has_pipeline_dependency)\n",
    "    else:\n",
    "      df = create_static(entity, has_pipeline_dependency)\n",
    "\n",
    "    if entity.select_expr:\n",
    "      logger.debug(f\"Applying select expression: {entity.select_expr}\")\n",
    "      df = df.selectExpr(*entity.select_expr)\n",
    "    return df\n",
    "\n",
    "def _create_quarantine_tables(\n",
    "    valid_name: str,\n",
    "    invalid_name: str,\n",
    "    quarantine_name: str,\n",
    "    entity: DeltaLiveEntity,\n",
    "):\n",
    "  @dlt.table(name=valid_name, partition_cols=entity.partition_cols)\n",
    "  def valid_data():\n",
    "    df: DataFrame = (\n",
    "        dlt.readStream(quarantine_name)\n",
    "        if entity.is_streaming and not has_scd(entity)\n",
    "        else dlt.read(quarantine_name)\n",
    "    )\n",
    "    return df.filter(f\"{QUARANTINE_COL}=false\").drop(QUARANTINE_COL, \"_rescued_data\")\n",
    "\n",
    "  @dlt.table(name=invalid_name, partition_cols=entity.partition_cols)\n",
    "  def invalid_data():\n",
    "    df: DataFrame = (\n",
    "        dlt.readStream(quarantine_name)\n",
    "        if entity.is_streaming and not has_scd(entity)\n",
    "        else dlt.read(quarantine_name)\n",
    "    )\n",
    "    return df.filter(f\"{QUARANTINE_COL}=true\").drop(QUARANTINE_COL)\n",
    "\n",
    "def _create_table(name: str, partition_cols: List[str], entity: DeltaLiveEntity):\n",
    "  logger.debug(f\"Creating table: {name}\")\n",
    "  entity_expectations: Expectations = entity.expectations\n",
    "  is_temporary: bool = entity.is_quarantined\n",
    "  has_pipeline_dependency: bool = entity.source_format == \"dlt\"\n",
    "\n",
    "  @dlt.table(\n",
    "    name=name,\n",
    "    schema=entity.source_schema,\n",
    "    comment=entity.comment,\n",
    "    partition_cols=partition_cols,\n",
    "    table_properties=entity.table_properties,\n",
    "    spark_conf=entity.spark_conf,\n",
    "    temporary=is_temporary,\n",
    "  )\n",
    "  @dlt.expect_all(expectations=entity_expectations.expect_all)\n",
    "  @dlt.expect_all_or_drop(expectations=entity_expectations.expect_all_or_drop)\n",
    "  @dlt.expect_all_or_fail(expectations=entity_expectations.expect_all_or_fail)\n",
    "  def target_table():\n",
    "    df: DataFrame = None\n",
    "    if entity.is_streaming:\n",
    "      df = create_streaming(entity, has_pipeline_dependency)\n",
    "    else:\n",
    "      df = create_static(entity, has_pipeline_dependency)\n",
    "\n",
    "    if entity.select_expr:\n",
    "      logger.debug(f\"Applying select expression: {entity.select_expr}\")\n",
    "      df = df.selectExpr(*entity.select_expr)\n",
    "\n",
    "    if can_quarantine(entity):\n",
    "      rules: str = quarantine_rules(entity)\n",
    "      df = df.withColumn(QUARANTINE_COL, F.expr(rules))\n",
    "    return df\n",
    "\n",
    "def create_streaming(entity, has_pipeline_dependency) -> DataFrame:\n",
    "  \"\"\"\n",
    "  Creates a streaming DataFrame based on the DeltaLiveEntity.\n",
    "\n",
    "  Returns:\n",
    "      DataFrame: The created streaming DataFrame.\n",
    "  \"\"\"\n",
    "  logger.info(f\"Creating streaming DataFrame for entity: {entity.entity_id}\")\n",
    "  df: DataFrame = None\n",
    "  if has_pipeline_dependency:\n",
    "    df = dlt.readStream(entity.source)\n",
    "  else:\n",
    "    df = (\n",
    "      spark.readStream.format(entity.source_format)\n",
    "                      .options(**entity.read_options)\n",
    "                      .load(entity.source)\n",
    "        )\n",
    "    \n",
    "  return df\n",
    "\n",
    "def create_static(entity, has_pipeline_dependency) -> DataFrame:\n",
    "  \"\"\"\n",
    "  Creates a static DataFrame based on the DeltaLiveEntity.\n",
    "\n",
    "  Returns:\n",
    "      DataFrame: The created static DataFrame.\n",
    "  \"\"\"\n",
    "  logger.info(f\"Creating static DataFrame for entity: {entity.entity_id}\")\n",
    "  df: DataFrame = None\n",
    "  if has_pipeline_dependency:\n",
    "    df = dlt.read(entity.source)\n",
    "  else:\n",
    "    df = (\n",
    "      spark.read.format(entity.source_format)\n",
    "                .options(**entity.read_options)\n",
    "                .load(entity.source)\n",
    "            )\n",
    "    \n",
    "  return df\n",
    "\n",
    "def _create_scd_table(\n",
    "    name: str, partition_cols: List[str], entity: DeltaLiveEntity\n",
    "):\n",
    "  logger.debug(f\"Creating SCD table: {name}\")\n",
    "  entity_expectations: Expectations = entity.expectations\n",
    "  dlt.create_streaming_table(\n",
    "      name=name,\n",
    "      schema=entity.source_schema,\n",
    "      comment=entity.comment,\n",
    "      partition_cols=partition_cols,\n",
    "      table_properties=entity.table_properties,\n",
    "      spark_conf=entity.spark_conf,\n",
    "      expect_all=entity_expectations.expect_all,\n",
    "      expect_all_or_drop=entity_expectations.expect_all_or_drop,\n",
    "      expect_all_or_fail=entity_expectations.expect_all_or_fail,\n",
    "  )\n",
    "  dlt.apply_changes(\n",
    "      target=name,\n",
    "      source=entity.source,\n",
    "      keys=entity.primary_keys,\n",
    "      sequence_by=entity.apply_changes.sequence_by,\n",
    "      where=entity.apply_changes.where,\n",
    "      ignore_null_updates=entity.apply_changes.ignore_null_updates,\n",
    "      apply_as_deletes=entity.apply_changes.apply_as_deletes,\n",
    "      apply_as_truncates=entity.apply_changes.apply_as_truncates,\n",
    "      column_list=entity.apply_changes.column_list,\n",
    "      except_column_list=entity.apply_changes.except_column_list,\n",
    "      stored_as_scd_type=entity.apply_changes.stored_as_scd_type,\n",
    "      track_history_column_list=entity.apply_changes.track_history_column_list,\n",
    "      track_history_except_column_list=entity.apply_changes.track_history_except_column_list,\n",
    "      flow_name=entity.apply_changes.flow_name,\n",
    "      ignore_null_updates_column_list=entity.apply_changes.ignore_null_updates_column_list,\n",
    "      ignore_null_updates_except_column_list=entity.apply_changes.ignore_null_updates_except_column_list,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger: Logger = create_logger(__name__)\n",
    "run()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "dlt_pipeline",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
