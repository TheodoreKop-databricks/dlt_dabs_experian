{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fded541c-c7ce-4a8f-825c-32474bec05c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import *\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import * \n",
    "from pyspark.sql.types import BooleanType, StructType, StructField, StringType, IntegerType, MapType, ArrayType\n",
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97590672-9f6f-4776-928f-f784e45801e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark_conf={\n",
    "        \"spark.databricks.delta.properties.defaults.autoOptimize.optimizeWrite\" : True,\n",
    "        \"spark.databricks.delta.properties.defaults.autoOptimize.autoCompact\" :  True,\n",
    "        \"spark.databricks.delta.optimizeWrite.enabled\": True,\n",
    "        \"spark.databricks.adaptive.autoOptimizeShuffle.enabled\": True,\n",
    "        \"spark.databricks.adaptive.skewJoin.spillProof.enabled\": True,\n",
    "        \"spark.sql.adaptive.skewJoin.enabled\": True,\n",
    "        \"spark.databricks.delta.cache.enabled\": True,\n",
    "        \"spark.databricks.io.cache.enabled\": True,\n",
    "        \"spark.sql.adaptive.coalescePartitions.enabled\": True,\n",
    "        \"spark.sql.shuffle.partitions\":\"auto\",\n",
    "        \"spark.databricks.optimizer.adaptive.enabled\":True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "500a4e42-9faf-48b4-b8f4-766eb4a6957b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "inicio = pd.Timestamp(\"2023-10-01\")\n",
    "\n",
    "last_60d = date.today() - timedelta(days = 61)\n",
    "print(last_60d)\n",
    "\n",
    "first_day = date.today().replace(day=1)\n",
    "# print(first_day)\n",
    "\n",
    "last_13m = first_day - timedelta(days = 365)\n",
    "print(last_13m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8dddbbe2-d168-44ad-9fdb-46d300cf2922",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### DLT METHODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b3a835f-9366-429c-8101-7e7b68cc6d49",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from dataclasses import asdict, dataclass, field\n",
    "from pyspark.sql import DataFrame\n",
    "from typing import Any, Callable, Dict, List, Union, Optional, Dict, Literal\n",
    "import dlt\n",
    "\n",
    "QUARANTINE_COL: str = \"is_quarantined\"\n",
    "\n",
    "def can_quarantine(is_quarantined: bool, expect_all: Dict[str, str]) -> bool:\n",
    "    quarantine: bool = is_quarantined and bool(expect_all) \n",
    "    #logger.debug(f\"Can quarantine: {quarantine}\")\n",
    "    return quarantine\n",
    "\n",
    "\n",
    "def quarantine_rules(is_quarantined: bool, expect_all: Dict[str, str]) -> str:\n",
    "    rules: str = (\n",
    "        \"NOT({0})\".format(\" AND \".join(expect_all.values()))\n",
    "        if can_quarantine(is_quarantined, expect_all)\n",
    "        else \"1=0\"\n",
    "    )\n",
    "    #logger.debug(f\"Quarantine rules: {rules}\")\n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e478a3f-79f5-4d43-8bc7-de1a801ea749",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_view(input_df: DataFrame, destination: str, comment: str, expect_all: Dict[str, str] = {}, expect_all_or_drop: Dict[str, str] = {}, expect_all_or_fail: Dict[str, str] ={}, spark_conf: Dict[str,str] = {}) -> None:\n",
    "  \"\"\"\n",
    "  Generates a view for the specified dataframe.\n",
    "  \"\"\"\n",
    "  #logger.info(f\"Generating view for entity: \")\n",
    "  @dlt.view(\n",
    "      name=destination,\n",
    "      comment=comment,\n",
    "      spark_conf=spark_conf\n",
    "  )\n",
    "  @dlt.expect_all(expectations=expect_all)\n",
    "  @dlt.expect_all_or_drop(expectations=expect_all_or_drop)\n",
    "  @dlt.expect_all_or_fail(expectations=expect_all_or_fail)\n",
    "  def _():\n",
    "    return input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c0eca04-2222-422d-a0de-c4f00781cb87",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_table(input_df: DataFrame, destination: str, partition_cols: List[str], comment: str, expect_all: Dict[str, str] = {}, expect_all_or_drop: Dict[str, str] = {}, expect_all_or_fail: Dict[str, str] = {}, is_quarantined: bool = False, has_scd: bool = False, src: str = '', keys: List[str] =[], sequence_by: str = '', ignore_null_updates: bool = False, apply_as_deletes: str = '', spark_conf: Dict[str,str] = {}) -> None:\n",
    "    \"\"\"\n",
    "    Generates a table for the specified entity.\n",
    "    \"\"\"\n",
    "    #logger.info(f\"Generating table for entity: \")\n",
    "    name: str = destination\n",
    "    quarantine_name: str = f\"{name}_quarantine\"\n",
    "    invalid_name: str = f\"{name}_invalid\"\n",
    "    if can_quarantine(is_quarantined, expect_all):\n",
    "        _create_quarantine_tables(\n",
    "            valid_name=name,\n",
    "            invalid_name=invalid_name,\n",
    "            quarantine_name=quarantine_name,\n",
    "            partition_cols = partition_cols)\n",
    "        name = quarantine_name\n",
    "        partition_cols = [QUARANTINE_COL] + partition_cols\n",
    "    if has_scd:\n",
    "        _create_scd_table(source=src, keys=keys, sequence_by=sequence_by, ignore_null_updates=ignore_null_updates, apply_as_deletes=apply_as_deletes, name=name, partition_cols=partition_cols, comment=comment, expect_all=expect_all, expect_all_or_drop=expect_all_or_drop, expect_all_or_fail=expect_all_or_fail, spark_conf = spark_conf)\n",
    "    else:\n",
    "        _create_table(input_df, name, partition_cols, comment, expect_all, expect_all_or_drop, expect_all_or_fail, is_quarantined, spark_conf = spark_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "afc93ac7-a1f9-437d-a6bd-9a32ec115a2e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def _create_quarantine_tables(\n",
    "    valid_name: str,\n",
    "    invalid_name: str,\n",
    "    quarantine_name: str,\n",
    "    partition_cols: List[str],\n",
    "):\n",
    "  @dlt.table(name=valid_name, partition_cols=partition_cols)\n",
    "  def valid_data():\n",
    "    df: DataFrame = (\n",
    "        dlt.readStream(quarantine_name)\n",
    "    )\n",
    "    return df.filter(f\"{QUARANTINE_COL}=false\").drop(QUARANTINE_COL, \"_rescued_data\")\n",
    "\n",
    "  @dlt.table(name=invalid_name, partition_cols=partition_cols)\n",
    "  def invalid_data():\n",
    "    df: DataFrame = (\n",
    "        dlt.readStream(quarantine_name)\n",
    "    )\n",
    "    return df.filter(f\"{QUARANTINE_COL}=true\").drop(QUARANTINE_COL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4d094ea-b03b-4fae-a8cf-150297a56676",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def _create_table(input_df: DataFrame, name: str, partition_cols: List[str], comment: str, expect_all: Dict, expect_all_or_drop: Dict, expect_all_or_fail: Dict, is_quarantined: bool, spark_conf: Dict):\n",
    "  #logger.debug(f\"Creating table: {name}\")\n",
    "  is_temporary: bool = is_quarantined\n",
    "  @dlt.table(\n",
    "    name=name,\n",
    "    comment=comment,\n",
    "    partition_cols=partition_cols,\n",
    "    spark_conf=spark_conf,\n",
    "    temporary=is_temporary,\n",
    "  )\n",
    "  @dlt.expect_all(expectations=expect_all)\n",
    "  @dlt.expect_all_or_drop(expectations=expect_all_or_drop)\n",
    "  @dlt.expect_all_or_fail(expectations=expect_all_or_fail)\n",
    "  def target_table():\n",
    "    df: DataFrame = input_df\n",
    "    if can_quarantine(is_quarantined, expect_all):\n",
    "      rules: str = quarantine_rules(is_quarantined, expect_all)\n",
    "      df = df.withColumn(QUARANTINE_COL, F.expr(rules))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3b99db8-b1a5-4afe-972f-d4b46883312c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def _create_scd_table(source:str, keys: List[str], sequence_by: str, ignore_null_updates: bool, apply_as_deletes: str, name: str, partition_cols: List[str], comment: str, expect_all: Dict, expect_all_or_drop: Dict, expect_all_or_fail: Dict, spark_conf: Dict\n",
    "):\n",
    "  #logger.debug(f\"Creating SCD table: {name}\")\n",
    "  dlt.create_streaming_table(\n",
    "      name=name,\n",
    "      comment=comment,\n",
    "      partition_cols=partition_cols,\n",
    "      spark_conf=spark_conf,\n",
    "      expect_all=expect_all,\n",
    "      expect_all_or_drop=expect_all_or_drop,\n",
    "      expect_all_or_fail=expect_all_or_fail,\n",
    "  )\n",
    "  dlt.apply_changes(\n",
    "      target=name,\n",
    "      source=source,\n",
    "      keys=keys,\n",
    "      sequence_by=sequence_by,\n",
    "      ignore_null_updates=ignore_null_updates,\n",
    "      apply_as_deletes=apply_as_deletes\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac461f14-321a-4927-9370-99c826363a2c",
     "showTitle": true,
     "title": "SIMULAÇÕES"
    }
   },
   "outputs": [],
   "source": [
    "a = f\"\"\"\n",
    "SELECT \n",
    "    cd_simulation,\n",
    "    consumer_cpf,\n",
    "    CAST(dt_created_at AS DATE)              AS dt_simulation,\n",
    "    date_format(dt_created_at, 'yyyyMM')     AS anomes,\n",
    "    consumerNegativated,\n",
    "    ds_client,\n",
    "    vl_simulation,\n",
    "\n",
    "    CASE\n",
    "    WHEN ds_client = \"ecred-serasa-web'\" THEN 'ecred-serasa-web' \n",
    "    ELSE ds_client \n",
    "    END AS canal,\n",
    "     \n",
    "    offerHallGatherKey,\n",
    "\n",
    "    CASE WHEN reason_key = 'credit-card' THEN 'Cartão'\n",
    "    ELSE 'Empréstimo'\n",
    "    END AS ds_product1,\n",
    "\n",
    "    CASE \n",
    "    WHEN cardinality(ls_products_events) >= 1 THEN 1 \n",
    "    ELSE                                    0 \n",
    "    END AS fl_elegivel_oferta,\n",
    "\n",
    "    CASE \n",
    "    WHEN cardinality(ls_products_events) >= 1 THEN 1 \n",
    "    ELSE                                    0 \n",
    "    END AS fl_chamado,\n",
    "\t\n",
    "    CASE \n",
    "    WHEN cardinality(ls_offers_events) >= 1 THEN 1 \n",
    "    ELSE                                  0 \n",
    "    END AS fl_offer\n",
    "    \n",
    "FROM db_ecred_silver.tb_new_events_simulation\n",
    "WHERE dt_event >= '{inicio}'\n",
    "      AND ds_client in ('ecred-web', 'ecred-app', 'app-ios', 'app-android', 'serasa-web')\n",
    "\"\"\"\n",
    "\n",
    "simulacao = spark.sql(a)\n",
    "generate_view(input_df=simulacao, comment= 'SOME RELEVANT COMMENT HERE', spark_conf=spark_conf, destination='temp_simulacao')\n",
    "\n",
    "#simulacao.createOrReplaceTempView('temp_simulacao')\n",
    "# simulacao.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2780cdb-fc22-4a92-b5bb-6613537e2007",
     "showTitle": true,
     "title": "BASE EVENTOS."
    }
   },
   "outputs": [],
   "source": [
    "df_eventos = spark.sql(f'''\n",
    "  SELECT *\n",
    "  FROM db_ecred_silver.tb_events_integration\n",
    "  WHERE dt_event >= '{inicio}'\n",
    "  AND cd_event IN (2600,2602,311)\n",
    "''')\n",
    "\n",
    "# df_eventos.createOrReplaceTempView('temp_df_eventos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "baa40f39-b746-4b6b-8016-debd021e1e6a",
     "showTitle": true,
     "title": "Schema evento 2600."
    }
   },
   "outputs": [],
   "source": [
    "schema_event_2600 = StructType([ \\\n",
    "    StructField(\"consumerScoresHSPN\",StringType(),True), \\\n",
    "    StructField(\"consumerProfessionLabel\",StringType(),True), \\\n",
    "    StructField(\"consumerOccupationLabel\",StringType(),True), \\\n",
    "    StructField(\"consumerAge\",StringType(),True), \\\n",
    "    StructField(\"consumerIncome\",StringType(),True), \\\n",
    "    StructField(\"consumerScoresHRP9\",StringType(),True), \\\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3231144-af52-4be5-99c2-bc1e7db419df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_event_2600 = df_eventos.filter(df_eventos.cd_event == 2600).withColumn(\"obj_ds_data\", from_json(df_eventos.ds_data, schema_event_2600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ba47eee-f0a3-4451-a697-c269ee6fa087",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_event_2600 = df_event_2600.withColumn(\"consumerScoresHSPN\", df_event_2600.obj_ds_data.consumerScoresHSPN)\\\n",
    "                             .withColumn(\"consumerProfessionLabel\", df_event_2600.obj_ds_data.consumerProfessionLabel)\\\n",
    "                             .withColumn(\"consumerOccupationLabel\", df_event_2600.obj_ds_data.consumerOccupationLabel)\\\n",
    "                             .withColumn(\"consumerAge\", df_event_2600.obj_ds_data.consumerAge)\\\n",
    "                             .withColumn(\"consumerIncome\", df_event_2600.obj_ds_data.consumerIncome)\\\n",
    "                             .withColumn(\"consumerScoresHRP9\", df_event_2600.obj_ds_data.consumerScoresHRP9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3be514a-9816-4918-b587-ca150082da17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "generate_view(input_df=df_event_2600, comment= 'SOME RELEVANT COMMENT HERE', spark_conf=spark_conf, destination='temp_df_event_2600')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e33ac74-719b-46a9-85ac-c3adb2472d69",
     "showTitle": true,
     "title": "Join EVENTO 2600"
    }
   },
   "outputs": [],
   "source": [
    "a = f\"\"\"\n",
    "SELECT\n",
    "    a.*,\n",
    "    CAST(b.consumerScoresHSPN AS int)      AS consumerScoresHSPN,\n",
    "    CAST(b.consumerAge AS int)             AS consumerAge,\n",
    "    CAST(b.consumerIncome AS int)          AS consumerIncome,\n",
    "    CAST(b.consumerScoresHRP9 AS int)      AS consumerScoresHRP9,\n",
    "    b.consumerProfessionLabel,\n",
    "    b.consumerOccupationLabel\n",
    "    -- ,CASE WHEN b.cd_simulation IS NULL THEN 0 ELSE 1 END AS fl_join_2600\n",
    "    \n",
    "FROM LIVE.temp_simulacao AS a\n",
    "  LEFT JOIN LIVE.temp_df_event_2600 AS b\n",
    "    ON a.cd_simulation = b.cd_simulation\n",
    "\"\"\"\n",
    "\n",
    "simulacao2 = spark.sql(a)\n",
    "\n",
    "generate_view(input_df=simulacao2, comment= 'SOME RELEVANT COMMENT HERE', spark_conf=spark_conf, destination='temp_simulacao2')\n",
    "#simulacao2.createOrReplaceTempView('temp_simulacao2')\n",
    "# simulacao2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ed49ff3-3377-43b8-b0b3-1b35a2ed9743",
     "showTitle": true,
     "title": "Schema evento 2602."
    }
   },
   "outputs": [],
   "source": [
    "schema_event_2602 = StructType([ \\\n",
    "    StructField(\"occupationLabel\",StringType(),True), \\\n",
    "    StructField(\"professionLabel\",StringType(),True), \\\n",
    "    StructField(\"addressStateValue\",StringType(),True), \\\n",
    "    StructField(\"addressCity\",StringType(),True), \\\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7943a055-13ef-48ef-b1ff-0d17922b8ce7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_event_2602 = df_eventos.filter(df_eventos.cd_event == 2602).withColumn(\"obj_ds_data\", from_json(df_eventos.ds_data, schema_event_2602))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d93cba0-1897-4eb3-ab44-ed45dcc0a332",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_event_2602 = df_event_2602.withColumn(\"occupationLabel\", df_event_2602.obj_ds_data.occupationLabel)\\\n",
    "                             .withColumn(\"professionLabel\", df_event_2602.obj_ds_data.professionLabel)\\\n",
    "                             .withColumn(\"addressStateValue\", df_event_2602.obj_ds_data.addressStateValue)\\\n",
    "                             .withColumn(\"addressCity\", df_event_2602.obj_ds_data.addressCity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bad54ad3-af70-4798-9540-eb6a13d1af22",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "generate_view(input_df=df_event_2602, comment= 'SOME RELEVANT COMMENT HERE', spark_conf=spark_conf, destination='temp_df_event_2602')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "791adcac-6919-478d-b27b-6b1596b49b48",
     "showTitle": true,
     "title": "Join EVENTO 2602"
    }
   },
   "outputs": [],
   "source": [
    "a = f\"\"\"\n",
    "SELECT\n",
    "    a.*,\n",
    "    b.addressStateValue\n",
    "    -- ,CASE WHEN b.cd_simulation IS NULL THEN 0 ELSE 1 END AS fl_join_2602\n",
    "    \n",
    "FROM LIVE.temp_simulacao2 AS a\n",
    "  LEFT JOIN LIVE.temp_df_event_2602 AS b\n",
    "    ON a.cd_simulation = b.cd_simulation\n",
    "\"\"\"\n",
    "\n",
    "simulacao3 = spark.sql(a)\n",
    "generate_view(input_df=simulacao3, comment= 'SOME RELEVANT COMMENT HERE', spark_conf=spark_conf, destination='temp_simulacao3')\n",
    "#simulacao3.createOrReplaceTempView('temp_simulacao3')\n",
    "# simulacao3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc9f0a39-df59-49a5-8e65-60b0c09c7c76",
     "showTitle": true,
     "title": "Schema evento 311."
    }
   },
   "outputs": [],
   "source": [
    "schema_event_311 = StructType([ \\\n",
    "    StructField(\"creditTypeKeyV2\",StringType(),True), \\\n",
    "    StructField(\"productId\",StringType(),True), \\\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "beb0bd2d-91ae-4dd3-91fc-2cebe1a38e40",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_event_311 = df_eventos.filter(df_eventos.cd_event == 311).withColumn(\"obj_ds_data\", from_json(df_eventos.ds_data, schema_event_311))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "040614ed-cc0b-4531-af31-37907d6e4f14",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_event_311 = df_event_311.withColumn(\"creditTypeKeyV2\", df_event_311.obj_ds_data.creditTypeKeyV2)\\\n",
    "                         .withColumn(\"productId\", df_event_311.obj_ds_data.productId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31a0a63a-6f45-480c-b291-ae6063ce546e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "generate_view(input_df=df_event_311, comment= 'SOME RELEVANT COMMENT HERE', spark_conf=spark_conf, destination='temp_df_event_311')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7512ee4e-c6cb-4774-a63e-71db244ffeaf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "a = f\"\"\"\n",
    "select\n",
    "cd_simulation,\n",
    "\n",
    "MAX(CASE WHEN creditTypeKeyV2 in ('personal-loan', 'personal-loan-fgts', 'vehicle-guarantee-loan', 'home-equity', 'payroll-loan') THEN 1 ELSE 0 END) AS fl_ep_311,\n",
    "MAX(CASE WHEN creditTypeKeyV2 in ('credit-card')                                                                                  THEN 1 ELSE 0 END) AS fl_cartao_311,\n",
    "MAX(CASE WHEN creditTypeKeyV2 in ('digital-account')                                                                              THEN 1 ELSE 0 END) AS fl_conta_311,\n",
    "\n",
    "MAX(CASE WHEN productId in ('118', '131', '136', '149', '161', '166', '169', '185', '186', '197', '199', '76')           THEN 1 ELSE 0 END) AS fl_cartao_gap,\n",
    "MAX(CASE WHEN productId in ('177')                                                                                       THEN 1 ELSE 0 END) AS fl_conta_gap,\n",
    "MAX(CASE WHEN productId in ('14', '145', '160', '171', '196', '29', '4', '162', '178', '189', '9', '188', '137', '198')  THEN 1 ELSE 0 END) AS fl_ep_gap\n",
    "\n",
    "FROM LIVE.temp_df_event_311\n",
    "GROUP BY 1\n",
    "\"\"\"\n",
    "\n",
    "aux_df_event_311 = spark.sql(a)\n",
    "generate_view(input_df=aux_df_event_311, comment= 'SOME RELEVANT COMMENT HERE', spark_conf=spark_conf, destination='temp_aux_df_event_311')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d199af0-8f3d-4d0d-9a81-d6871c7f0f4f",
     "showTitle": true,
     "title": "Join EVENTO 311"
    }
   },
   "outputs": [],
   "source": [
    "a = f\"\"\"\n",
    "SELECT\n",
    "    a.*,\n",
    "    b.fl_ep_311,\n",
    "    b.fl_cartao_311,\n",
    "    b.fl_conta_311,\n",
    "    b.fl_cartao_gap,\n",
    "    b.fl_conta_gap,\n",
    "    b.fl_ep_gap\n",
    "    -- ,CASE WHEN b.cd_simulation IS NULL THEN 0 ELSE 1 END AS fl_join_311\n",
    "    \n",
    "FROM LIVE.temp_simulacao3 AS a\n",
    "  LEFT JOIN LIVE.temp_aux_df_event_311 AS b\n",
    "    ON a.cd_simulation = b.cd_simulation\n",
    "\"\"\"\n",
    "\n",
    "simulacao4 = spark.sql(a)\n",
    "generate_view(input_df=simulacao4, comment= 'SOME RELEVANT COMMENT HERE', spark_conf=spark_conf, destination='temp_simulacao4')\n",
    "#simulacao4.createOrReplaceTempView('temp_simulacao4')\n",
    "# simulacao4.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72ff71e4-d071-4d57-87ad-369aca94d849",
     "showTitle": true,
     "title": "Cria faixas"
    }
   },
   "outputs": [],
   "source": [
    "a = f\"\"\"\n",
    "SELECT\n",
    "*,   \n",
    "\n",
    "CASE WHEN ds_product1 = 'Cartão'     THEN 1 ELSE 0 END AS fl_cartao_reason,\n",
    "CASE WHEN ds_product1 = 'Empréstimo' THEN 1 ELSE 0 END AS fl_ep_reason,\n",
    "0 AS fl_conta_reason,\n",
    "\n",
    "CASE WHEN consumerScoresHSPN > 900                              THEN '01. 900+'\n",
    "     WHEN consumerScoresHSPN > 800 and consumerScoresHSPN <=900 THEN '02. 801-900'\n",
    "     WHEN consumerScoresHSPN > 700 and consumerScoresHSPN <=800 THEN '03. 701-800'\n",
    "     WHEN consumerScoresHSPN > 600 and consumerScoresHSPN <=700 THEN '04. 601-700'\n",
    "     WHEN consumerScoresHSPN > 500 and consumerScoresHSPN <=600 THEN '05. 501-600'\n",
    "     WHEN consumerScoresHSPN > 400 and consumerScoresHSPN <=500 THEN '06. 401-500'\n",
    "     WHEN consumerScoresHSPN > 300 and consumerScoresHSPN <=400 THEN '07. 301-400'\n",
    "     WHEN consumerScoresHSPN > 200 and consumerScoresHSPN <=300 THEN '08. 201-300'\n",
    "     WHEN consumerScoresHSPN > 100 and consumerScoresHSPN <=200 THEN '09. 101-200'\n",
    "     WHEN consumerScoresHSPN > 0   and consumerScoresHSPN <=100 THEN '10. 0-100'\n",
    "     ELSE null\n",
    "END AS fx_consumerScoresHSPN,\n",
    "\n",
    "CASE WHEN consumernegativated = 'true' THEN 'Negativado'\n",
    "     WHEN consumerScoresHSPN < 700    THEN '0 a 699'\n",
    "     WHEN consumerScoresHSPN >= 700   THEN '700+'\n",
    "END AS qualidade,\n",
    "\n",
    "CASE WHEN consumerScoresHSPN >= 0  and consumerScoresHSPN <= 300 THEN 'G04 - Negativado e baixíssima renda'\n",
    "     WHEN consumerScoresHSPN > 300 and consumerScoresHSPN <= 600 THEN 'G03 - Negativado com baixa renda mas com capacidade de pagamento'\n",
    "     WHEN consumerScoresHSPN > 600 and consumerScoresHSPN <= 800 THEN 'G02 - Não Negativado com renda média e alta capacidade de pontualidade em pagamentos'\n",
    "     WHEN consumerScoresHSPN > 800                               THEN 'G01 - Não Negativado com maior renda e alta capacidade de pontualidade em pagamentos'\n",
    "ELSE 'NA'\n",
    "END AS cluster_hspn_puro,\n",
    "\n",
    "CASE WHEN consumerScoresHRP9 >= 20000                                THEN  '07. 20k+'\n",
    "     WHEN consumerScoresHRP9 >= 10000 and consumerScoresHRP9 < 20000 THEN  '06. 10k-20k'\n",
    "     WHEN consumerScoresHRP9 >= 8000  and consumerScoresHRP9 < 10000 THEN  '05. 8k-10k'\n",
    "     WHEN consumerScoresHRP9 >= 6000  and consumerScoresHRP9 < 8000  THEN  '04. 6k-8k'\n",
    "     WHEN consumerScoresHRP9 >= 4000  and consumerScoresHRP9 < 6000  THEN  '03. 4k-6k'\n",
    "     WHEN consumerScoresHRP9 >= 2000  and consumerScoresHRP9 < 4000  THEN  '02. 2k-4k'\n",
    "     WHEN consumerScoresHRP9 >= 0     and consumerScoresHRP9 < 2000  THEN  '01. 0-2k'   \n",
    "     ELSE null\n",
    "END AS fx_consumerScoresHRP9,\n",
    "\n",
    "CASE WHEN consumerIncome >= 20000                            THEN  '07. 20k+'\n",
    "     WHEN consumerIncome >= 10000 and consumerIncome < 20000 THEN  '06. 10k-20k'\n",
    "     WHEN consumerIncome >= 8000  and consumerIncome < 10000 THEN  '05. 8k-10k'\n",
    "     WHEN consumerIncome >= 6000  and consumerIncome < 8000  THEN  '04. 6k-8k'\n",
    "     WHEN consumerIncome >= 4000  and consumerIncome < 6000  THEN  '03. 4k-6k'\n",
    "     WHEN consumerIncome >= 2000  and consumerIncome < 4000  THEN  '02. 2k-4k'\n",
    "     WHEN consumerIncome >= 0     and consumerIncome < 2000  THEN  '01. 0-2k'   \n",
    "     ELSE null\n",
    "END AS fx_consumerIncome,\n",
    "\n",
    "CASE WHEN consumerAge >= 60                      THEN '06. 60+'\n",
    "     WHEN consumerAge >= 50 and consumerAge < 60 THEN '05. 50-59'\n",
    "     WHEN consumerAge >= 40 and consumerAge < 50 THEN '04. 40-49'\n",
    "     WHEN consumerAge >= 30 and consumerAge < 40 THEN '03. 30-39'\n",
    "     WHEN consumerAge >= 20 and consumerAge < 30 THEN '02. 20-29'\n",
    "     WHEN consumerAge >= 0 and consumerAge < 20  THEN '01. 0-19'\n",
    "     ELSE null\n",
    "END AS fx_consumerAge,\n",
    "\n",
    "CASE WHEN addressStateValue IS NULL                                             THEN 'Null'\n",
    "     WHEN addressStateValue IN (\"DF\",\"GO\",\"MS\",\"MT\") \t\t\t\t\t\tTHEN 'CO'\n",
    "     WHEN addressStateValue IN (\"AC\",\"AM\",\"AP\",\"PA\",\"RO\",\"RR\",\"TO\") \t          THEN 'N'\n",
    "     WHEN addressStateValue IN (\"AL\",\"BA\",\"CE\",\"MA\",\"PB\",\"PE\",\"PI\",\"RN\",\"SE\")   THEN 'NE'\n",
    "     WHEN addressStateValue IN (\"ES\",\"MG\",\"RJ\",\"SP\") \t\t\t\t\t\tTHEN 'SE'\n",
    "     WHEN addressStateValue IN (\"PR\",\"RS\",\"SC\") \t\t\t\t\t\t\tTHEN 'SUL'\n",
    "     ELSE                                                                            'nda'\n",
    "END AS regiao\n",
    "    \n",
    "FROM LIVE.temp_simulacao4\n",
    "\"\"\"\n",
    "\n",
    "simulacao5 = spark.sql(a)\n",
    "generate_view(input_df=simulacao5, comment= 'SOME RELEVANT COMMENT HERE', spark_conf=spark_conf, destination='temp_df_nivel_simulacao5')\n",
    "#simulacao5.createOrReplaceTempView('temp_df_nivel_simulacao5')\n",
    "# simulacao5.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db52c32d-dd70-4dac-bb50-eaa2393aad0d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "a = f\"\"\"\n",
    "select\n",
    "  cd_simulation,\n",
    "  consumer_cpf,\n",
    "  dt_simulation,\n",
    "  anomes,\n",
    "  consumerNegativated,\n",
    "  ds_client,\n",
    "  vl_simulation,\n",
    "  canal,\n",
    "  offerHallGatherKey,\n",
    "  fl_elegivel_oferta,\n",
    "  fl_chamado,\n",
    "  fl_offer,\n",
    "  consumerScoresHSPN,\n",
    "  consumerAge,\n",
    "  consumerIncome,\n",
    "  consumerScoresHRP9,\n",
    "  consumerProfessionLabel,\n",
    "  consumerOccupationLabel,\n",
    "  addressStateValue,\n",
    "  fx_consumerScoresHSPN,\n",
    "  qualidade,\n",
    "  cluster_hspn_puro,\n",
    "  fx_consumerScoresHRP9,\n",
    "  fx_consumerIncome,\n",
    "  fx_consumerAge,\n",
    "  regiao,\n",
    "\n",
    "  CASE WHEN dt_simulation < '2023-10-20'                       THEN fl_cartao_reason\n",
    "      WHEN dt_simulation between '2023-10-20' and '2023-11-09' THEN fl_cartao_gap\n",
    "      ELSE                                                          fl_cartao_311\n",
    "  END AS fl_cartao,\n",
    "\n",
    "  CASE WHEN dt_simulation < '2023-10-20'                       THEN fl_conta_reason\n",
    "      WHEN dt_simulation between '2023-10-20' and '2023-11-09' THEN fl_conta_gap\n",
    "      ELSE                                                          fl_conta_311\n",
    "  END AS fl_conta,\n",
    "\n",
    "  CASE WHEN dt_simulation < '2023-10-20'                       THEN fl_ep_reason\n",
    "      WHEN dt_simulation between '2023-10-20' and '2023-11-09' THEN fl_ep_gap\n",
    "      ELSE                                                          fl_ep_311\n",
    "  END AS fl_ep\n",
    "\n",
    "FROM LIVE.temp_df_nivel_simulacao5\n",
    "\"\"\"\n",
    "\n",
    "simulacao6 = spark.sql(a)\n",
    "generate_view(input_df=simulacao6, comment= 'SOME RELEVANT COMMENT HERE', spark_conf=spark_conf, destination='temp_simulacao6')\n",
    "#simulacao6.createOrReplaceTempView('temp_simulacao6')\n",
    "# simulacao6.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5bf90ea-58de-48ca-a176-008ad30f09e6",
     "showTitle": true,
     "title": "PEDIDOS"
    }
   },
   "outputs": [],
   "source": [
    "a = f\"\"\"\n",
    "SELECT DISTINCT\n",
    "    cd_simulation_id,\n",
    "    nu_document,\n",
    "    cd_uuid,\n",
    "    cd_order,\n",
    "    ds_key,\n",
    "    SUBSTR(dt_created, 1, 7)    AS anomes,\n",
    "    CAST(dt_created as DATE)    AS dt_pedido,\n",
    "    ds_client                   AS canal,\n",
    "    vl_order,\n",
    "    vl_contracted, \n",
    "    \n",
    "    CASE\n",
    "    WHEN ds_order_status_key NOT IN ('waiting-for-review',\n",
    "                                     'cancelled-by-address-changed',\n",
    "                                     'expired',\n",
    "                                     'waiting-for-answers',\n",
    "                                     'waiting-for-redirect',\n",
    "                                     'waiting-for-phone-validation',\n",
    "                                     'waiting-for-bank-validation',\n",
    "                                     'expired-review',\n",
    "                                     'expired-answers',\n",
    "                                     'expired-bank-validation',\n",
    "                                     'expired-phone-validation',\n",
    "                                     'expired-review-offer',\n",
    "                                     'expired-redirect',\n",
    "                                     'invalid-bank-account') THEN 1 \n",
    "    ELSE                                                          0 \n",
    "    END AS flag_confirmado,\n",
    "    CASE WHEN ds_order_status IN ('Aprovado', 'Liberado') THEN 1 \n",
    "    ELSE                                                       0 \n",
    "    END AS flag_aprovado,\n",
    "    \n",
    "    CASE \n",
    "    WHEN ds_credit_type LIKE 'Cart%' THEN 'Cartão'\n",
    "    WHEN ds_credit_type LIKE 'Empr%' THEN 'Empréstimo'\n",
    "    ELSE \t\t\t\t\t\t\t\t  'Outros' \n",
    "    END AS ds_product,\n",
    "    \n",
    "    cd_credit_type,\n",
    "    ds_credit_type_key,\n",
    "    ds_credit_type,\n",
    "    ds_variant\n",
    "    \n",
    "FROM db_ecred_silver.tb_order_report\n",
    "WHERE CAST(dt_created AS DATE) >= '{inicio}'\n",
    "\"\"\"\n",
    "\n",
    "df_pedidos = spark.sql(a)\n",
    "generate_view(input_df=df_pedidos, comment= 'SOME RELEVANT COMMENT HERE', spark_conf=spark_conf, destination='temp_tbl_pedidos')\n",
    "#df_pedidos.createOrReplaceTempView('temp_tbl_pedidos')\n",
    "# df_pedidos.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5ecc76b-f463-48a8-b263-b2b11c0634a4",
     "showTitle": false,
     "title": "5. Cruza bases de Simulação e Pedido para gerar Funil."
    }
   },
   "outputs": [],
   "source": [
    "a = f\"\"\"\n",
    "SELECT DISTINCT\n",
    "    a.*,\n",
    "    b.ds_key,\n",
    "    b.vl_order,\n",
    "\n",
    "    CASE \n",
    "    WHEN b.flag_aprovado = 1 AND b.vl_contracted > 0  THEN b.vl_contracted \n",
    "    WHEN b.flag_aprovado = 1 AND b.vl_contracted <= 0 THEN b.vl_order\n",
    "    ELSE                                                   null\n",
    "    END AS vl_liberado,\n",
    "\n",
    "    CASE WHEN b.cd_order        IS NOT NULL THEN 1 ELSE 0                 END AS flag_pedido,\n",
    "    CASE WHEN b.flag_confirmado IS NULL     THEN 0 ELSE b.flag_confirmado END AS flag_confirmado,\n",
    "    CASE WHEN b.flag_aprovado   IS NULL     THEN 0 ELSE b.flag_aprovado   END AS flag_aprovado\n",
    "    \n",
    "FROM LIVE.temp_simulacao6 AS a\n",
    "    LEFT JOIN LIVE.temp_tbl_pedidos AS b\n",
    "    ON a.cd_simulation = b.cd_simulation_id\n",
    "    --AND a.partner_Key = b.ds_key\n",
    "    --AND a.creditTypeKey = b.ds_credit_type_key\n",
    "\"\"\"\n",
    "\n",
    "simulacao7 = spark.sql(a)\n",
    "generate_view(input_df=simulacao7, comment= 'SOME RELEVANT COMMENT HERE', spark_conf=spark_conf, destination='temp_simulacao7')\n",
    "#simulacao7.createOrReplaceTempView('temp_simulacao7')\n",
    "# simulacao7.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a4e39ce-e967-4ac2-b4de-0f6c6b6a88e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "a = f\"\"\"\n",
    "SELECT DISTINCT\n",
    "anomes,\n",
    "dt_simulation,\n",
    "offerHallGatherKey,\n",
    "fl_ep,\n",
    "fl_cartao,\n",
    "fl_conta,\n",
    "ds_client,\n",
    "-- partner_key,                                     (SÓ PARA FUNIL POR PARCEIRO)\n",
    "-- description,                                     (SÓ PARA FUNIL POR SUBPRODUTO)\n",
    "fx_consumerScoresHSPN,\n",
    "qualidade,\n",
    "fx_consumerScoresHRP9,\n",
    "fx_consumerIncome,\n",
    "fx_consumerAge,\n",
    "regiao,\n",
    "consumerOccupationLabel,\n",
    "cluster_hspn_puro,\n",
    "cd_simulation,\n",
    "fl_elegivel_oferta,\n",
    "fl_chamado,\n",
    "fl_offer,\n",
    "flag_pedido,\n",
    "flag_confirmado,\n",
    "flag_aprovado\n",
    "\n",
    "from LIVE.temp_simulacao7\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "df_analitico = spark.sql(a)\n",
    "generate_view(input_df=df_analitico, comment= 'SOME RELEVANT COMMENT HERE', spark_conf=spark_conf, destination='temp_df_analitico')\n",
    "#df_analitico.createOrReplaceTempView('temp_df_analitico')\n",
    "# df_analitico.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "167b668a-28e2-413d-93cb-63d56f0689f4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### GERA ANALÍTICO PARA JOIN DO 1° CÓDIGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95fc2cb3-7fcd-42bf-8698-1b836c2ada8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "generate_table(input_df=df_analitico, comment= 'SOME RELEVANT COMMENT HERE', destination='dash_vida_macroproduto_analitico_events2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58e2b1b3-661e-4f8c-934d-f995148bd21c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "a = f\"\"\"\n",
    "select * from db_ecred.dash_vida_macroproduto_analitico_events1\n",
    "UNION all\n",
    "select * from LIVE.dash_vida_macroproduto_analitico_events2\n",
    "\"\"\"\n",
    "df_analitico_events_final = spark.sql(a)\n",
    "generate_table(input_df=df_analitico_events_final, comment= 'SOME RELEVANT COMMENT HERE', destination='dash_vida_macroproduto_analitico_events_final')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f4cb310-7d1a-4c57-8c83-1633cb4b0fcd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### MARCAÇÃO DE DATAS E GERAÇÃO DO CUBO FINAL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03797f80-a049-471d-be2a-a938775a5588",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "a = f\"\"\"\n",
    "SELECT\n",
    "    *,\n",
    "    CASE\n",
    "    WHEN dt_simulation >= '{last_60d}' THEN 1\n",
    "    ELSE                                    0\n",
    "    END AS last_60d,\n",
    "\n",
    "    CASE\n",
    "    WHEN dt_simulation < '{last_60d}' THEN NULL\n",
    "    ELSE                                   dt_simulation\n",
    "    END AS dt_simulation_60d,\n",
    "\n",
    "    CASE\n",
    "    WHEN dt_simulation >= '{last_13m}' THEN 1\n",
    "    ELSE                                    0\n",
    "    END AS last_13m\n",
    "\n",
    "FROM db_ecred.dash_vida_macroproduto_analitico_events_final\n",
    "\"\"\"\n",
    "\n",
    "df_funil_produto_datas = spark.sql(a)\n",
    "generate_view(input_df=df_funil_produto_datas, comment= 'SOME RELEVANT COMMENT HERE', destination='temp_df_funil_produto_datas')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e7eeb0d-0e07-4440-8c2d-748ca61c3894",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "a= f\"\"\"\n",
    "SELECT\n",
    "anomes as anomes_simulacao,\n",
    "dt_simulation_60d,\n",
    "last_60d,\n",
    "last_13m,\n",
    "offerHallGatherKey,\n",
    "fl_ep,\n",
    "fl_cartao,\n",
    "fl_conta,\n",
    "ds_client,\n",
    "fx_consumerScoresHSPN,\n",
    "qualidade,\n",
    "fx_consumerScoresHRP9,\n",
    "fx_consumerIncome,\n",
    "fx_consumerAge,\n",
    "regiao,\n",
    "consumerOccupationLabel,\n",
    "cluster_hspn_puro,\n",
    "COUNT(cd_simulation)        as qt_simulacao,\n",
    "SUM(fl_elegivel_oferta)     as qt_elegivel,\n",
    "SUM(fl_chamado)             as qt_chamado,\n",
    "SUM(fl_offer)               as qt_oferta,\n",
    "SUM(flag_pedido)            as qt_pedido,\n",
    "SUM(flag_confirmado)        as qt_confirmado,\n",
    "SUM(flag_aprovado)          as qt_aprovado\n",
    "\n",
    "from LIVE.temp_df_funil_produto_datas\n",
    "group by 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17\n",
    "\"\"\"\n",
    "df_dash_vida_tableau_macroproduto = spark.sql(a)\n",
    "generate_table(input_df=df_dash_vida_tableau_macroproduto, comment= 'SOME RELEVANT COMMENT HERE', destination='dash_vida_tableau_macroproduto')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DASH DA VIDA - MACROPRODUTO (>= Out23) -DLT",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
